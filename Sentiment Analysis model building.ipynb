{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Sentiment Analysis Model for the Web App\n",
    "We shall build this model using the following few steps;\n",
    "1. Load the dataset\n",
    "2. Define a Preprocessor and a Lemmatizer function\n",
    "3. Building the model\n",
    "4. Train our model\n",
    "5. Validate and save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import some important general package , pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Used to load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the dataset\n",
    "Download the dataset into a `data` folder by running the cell below.This dataset is available at http://thinknook.com/Twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-24 02:05:10--  http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip\n",
      "Resolving thinknook.com (thinknook.com)... 208.109.47.128\n",
      "Connecting to thinknook.com (thinknook.com)|208.109.47.128|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 56427677 (54M) [application/zip]\n",
      "Saving to: ‘./data/Sentiment-Analysis-Dataset.zip’\n",
      "\n",
      "Sentiment-Analysis- 100%[===================>]  53.81M  1.24MB/s    in 52s     \n",
      "\n",
      "2020-12-24 02:06:03 (1.04 MB/s) - ‘./data/Sentiment-Analysis-Dataset.zip’ saved [56427677/56427677]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data  # creating a folder to store data\n",
    "!wget http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip -nc -P ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the dataset\n",
    "Load the dataset using ``pd.read_csv`` and assign it to a variable ``df`` from file path ``data/Sentimet-Analysis-Dataset.zip``. Note that parameter ``compression = 'zip'`` because the dataset is a ``.zip`` file so it tells ``pandas`` that the file is zipped and it handles it like that. ``error_bad_lines = False`` because there are some raws in the dataset with more columns/fields and so this parameter tells ``pandas`` to ignore such raws and proceed to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv('data/Sentiment-Analysis-Dataset.zip',compression='zip',error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are only two raws with more fields and they have been left out and this is not bad because we still have a million more raws of data to work with.Use `df.head()` to view the dataframe and use `len(df)` to know the number of raws left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raws in dataset: 1578612\n"
     ]
    }
   ],
   "source": [
    "print('Number of raws in dataset: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Sunny Again        Work Tomorrow  :-|  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>handed in my uniform today . i miss you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "5       6          0    Sentiment140   \n",
       "6       7          1    Sentiment140   \n",
       "7       8          0    Sentiment140   \n",
       "8       9          1    Sentiment140   \n",
       "9      10          1    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  \n",
       "5                  or i just worry too much?          \n",
       "6                 Juuuuuuuuuuuuuuuuussssst Chillin!!  \n",
       "7         Sunny Again        Work Tomorrow  :-|  ...  \n",
       "8        handed in my uniform today . i miss you ...  \n",
       "9           hmmmm.... i wonder how she my number @-)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)# showing first 10 raws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our dataframe, we shall use the `Sentiment` column and the `SentimentText` column.But has you can see, some texts in the `sentimentText` column have slungs and some characters like '&lt' and so we must deal with such kind of data so that our model learns well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a Preprocessor and a Lemmatizer function\n",
    "What this function does is that it takes in a document`doc` searches for all characters and converts them to there English meanings forexample `&lt` is converted to  `<`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import unescape\n",
    "def preprocessor(doc):\n",
    "    #Takes in a document (a raw from the SentimentText column)\n",
    "    return unescape(doc).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor('&lt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `spacy` package which is used in Natural Langauge processing and import the english processor using `en_core_web_sm` parameter. `STOP_WORDS` are just words that our model does not give much weight to during training because they usually don't carry much meaning. Note parameter `disable = ['rer,'parser','tagger']` to disable some functions performed by `spacy` to speed up the process because we don't need those functions for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets load the english natural lang processor and disable some functions to make it faster\n",
    "import spacy\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_sm',disable=['rer','parser','tagger'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lemmatizer takes in a document/sentences `doc` and returns a lemma for each word forexample, the lemma for running is run which still means the same thing though shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a lemmatizer function\n",
    "def lemmatizer(doc):\n",
    "    return [word.lemma_ for word in nlp(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create stop words lemma since we shall use lemmas in our training document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create our stop words lemma\n",
    "STOP_WORDS_lemma = [word.lemma_ for word in nlp(\" \".join(list(STOP_WORDS)))]\n",
    "#Add ',','.'and ';' to stop words\n",
    "STOP_WORDS_lemma = set(STOP_WORDS_lemma).union(['.',';',','])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Building the model\n",
    "we shall use a naive bayes model because our interest is to build a model that returns the probability of a sentiment being positive. There are other probability models that you can try out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets build our model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the `TfidfVectorizer` and comment the `HashingVectorizer` for more accuracy but the problem with doing that is that your model will take more time to train and score. We shall use a `Pipeline` to just organize our process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(preprocessor=preprocessor,\n",
    "#                             tokenizer=lemmatizer,\n",
    "#                             ngram_range=(1,2),\n",
    "#                             stop_words=STOP_WORDS_lemma)\n",
    "vectorizer = HashingVectorizer(preprocessor = preprocessor,\n",
    "#                             tokenizer=lemmatizer,\n",
    "                               alternate_sign = False,\n",
    "#                             ngram_range=(1,2),\n",
    "                            stop_words=STOP_WORDS)\n",
    "clf = MultinomialNB()\n",
    "model = Pipeline([('vectorizer',vectorizer),\n",
    "                 ('classifier',clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split our data into train and test data using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets split our data into train and test\n",
    "X = df['SentimentText']\n",
    "y = df['Sentiment'] \n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invitech/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 HashingVectorizer(alternate_sign=False,\n",
       "                                   preprocessor=<function preprocessor at 0x7f897a9f2b90>,\n",
       "                                   stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                               \"'ve\", 'a', 'about', 'above',\n",
       "                                               'across', 'after', 'afterwards',\n",
       "                                               'again', 'against', 'all',\n",
       "                                               'almost', 'alone', 'along',\n",
       "                                               'already', 'also', 'although',\n",
       "                                               'always', 'am', 'among',\n",
       "                                               'amongst', 'amount', 'an', 'and',\n",
       "                                               'another', 'any', ...})),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets train our model\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8073322358497065"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check model accuracy on training data\n",
    "model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Validate and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7699090658583632"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check model accuracy on test data\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our model has a 77% accuracy which is not so bad atleast, so we now have to zip and save it so that we can use it to build our sentiment wed app.\n",
    "We shall use `dill` to save the model and `gzip` to compress the model to reduce it's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import dill\n",
    "\n",
    "with gzip.open('SentimentModel.dill.gz','wb') as f:\n",
    "    dill.dump(model,f,recurse=True)#recurse = True to make sure all the parameters are saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the model from the save file `SentimentModel.dill.gz` to be sure that it works the same and test it's performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import dill\n",
    "\n",
    "with gzip.open('SentimentModel.dill.gz','rb') as f:\n",
    "    sentiment_model = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invitech/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7699090658583632"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
